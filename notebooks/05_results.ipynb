{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenreBot : Using Natural Language Processing in Music Marketing\n",
    "\n",
    "For background on this project, please see the [README](../README.md).\n",
    "\n",
    "**Notebooks**\n",
    "- [Data Acquisition](./01_data_acquisition.ipynb)\n",
    "- [Data Cleaning](./02_data_cleaning.ipynb)\n",
    "- [Exploratory Data Analysis](./03_eda.ipynb)\n",
    "- [Modeling](./04_modeling.ipynb)\n",
    "- [Experiments](./04a_experiments.ipynb)\n",
    "- Results and Recommendations (this notebook))\n",
    "\n",
    "**In this notebook, you'll find:**\n",
    "- Comparison of our models and selection of a production model\n",
    "- Application of production model to our dataset for final classifications\n",
    "- A quick look at a couple of misclassified documents\n",
    "- Extraction of top terms based on classifications\n",
    "- Conclusions/next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# We'll see why we need this in a minute!\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# infrastructure\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# supervised\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# heterogeneous-model supervised\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + LogisticRegression</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.975254</td>\n",
       "      <td>0.931795</td>\n",
       "      <td>0.912048</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.930356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer + LogisticRegression</td>\n",
       "      <td>1.495226</td>\n",
       "      <td>0.956996</td>\n",
       "      <td>0.936810</td>\n",
       "      <td>0.917671</td>\n",
       "      <td>0.954071</td>\n",
       "      <td>0.935517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer + tuned LogisticRegression</td>\n",
       "      <td>4.055915</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.938014</td>\n",
       "      <td>0.918474</td>\n",
       "      <td>0.955704</td>\n",
       "      <td>0.936719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF + MultinomialNB</td>\n",
       "      <td>1.296156</td>\n",
       "      <td>0.941413</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.929810</td>\n",
       "      <td>0.927752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer + Bagging</td>\n",
       "      <td>28.872257</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.906319</td>\n",
       "      <td>0.884739</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.904166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer + Random Forest</td>\n",
       "      <td>2.628158</td>\n",
       "      <td>0.914460</td>\n",
       "      <td>0.903310</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.960128</td>\n",
       "      <td>0.896832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TfidfVectorizer + ExtraTrees</td>\n",
       "      <td>1.858499</td>\n",
       "      <td>0.921750</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.910387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer + AdaBoost</td>\n",
       "      <td>7.388280</td>\n",
       "      <td>0.900482</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.952206</td>\n",
       "      <td>0.888127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TfidfVectorizer + GradientBoost</td>\n",
       "      <td>2.421923</td>\n",
       "      <td>0.933119</td>\n",
       "      <td>0.924574</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.962380</td>\n",
       "      <td>0.921273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer + LogReg/MNB/Grad Stack</td>\n",
       "      <td>15.633441</td>\n",
       "      <td>0.952782</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.936878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TfidfVectorizer + MNB/Grad stack</td>\n",
       "      <td>4.415673</td>\n",
       "      <td>0.945693</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.922892</td>\n",
       "      <td>0.939877</td>\n",
       "      <td>0.931307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer + Logreg/Grad stack</td>\n",
       "      <td>14.953498</td>\n",
       "      <td>0.957665</td>\n",
       "      <td>0.937212</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>0.950352</td>\n",
       "      <td>0.936214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer + Logreg/MNB stack</td>\n",
       "      <td>10.692293</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.941424</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.950041</td>\n",
       "      <td>0.940795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model   fit_time  train_acc  \\\n",
       "0         CountVectorizer + LogisticRegression   0.874005   0.975254   \n",
       "1         TfidfVectorizer + LogisticRegression   1.495226   0.956996   \n",
       "2   TfidfVectorizer + tuned LogisticRegression   4.055915   0.955257   \n",
       "3                        TFIDF + MultinomialNB   1.296156   0.941413   \n",
       "4                    TfidfVectorizer + Bagging  28.872257   0.992376   \n",
       "5              TfidfVectorizer + Random Forest   2.628158   0.914460   \n",
       "6                 TfidfVectorizer + ExtraTrees   1.858499   0.921750   \n",
       "7                   TfidfVectorizer + AdaBoost   7.388280   0.900482   \n",
       "8              TfidfVectorizer + GradientBoost   2.421923   0.933119   \n",
       "9      TfidfVectorizer + LogReg/MNB/Grad Stack  15.633441   0.952782   \n",
       "10            TfidfVectorizer + MNB/Grad stack   4.415673   0.945693   \n",
       "11         TfidfVectorizer + Logreg/Grad stack  14.953498   0.957665   \n",
       "12          TfidfVectorizer + Logreg/MNB stack  10.692293   0.957263   \n",
       "\n",
       "    test_acc    recall  precision        f1  \n",
       "0   0.931795  0.912048   0.949415  0.930356  \n",
       "1   0.936810  0.917671   0.954071  0.935517  \n",
       "2   0.938014  0.918474   0.955704  0.936719  \n",
       "3   0.927984  0.925703   0.929810  0.927752  \n",
       "4   0.906319  0.884739   0.924465  0.904166  \n",
       "5   0.903310  0.841365   0.960128  0.896832  \n",
       "6   0.911735  0.897590   0.923554  0.910387  \n",
       "7   0.895286  0.832129   0.952206  0.888127  \n",
       "8   0.924574  0.883534   0.962380  0.921273  \n",
       "9   0.937613  0.926908   0.947066  0.936878  \n",
       "10  0.931996  0.922892   0.939877  0.931307  \n",
       "11  0.937212  0.922490   0.950352  0.936214  \n",
       "12  0.941424  0.931727   0.950041  0.940795  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our model stats\n",
    "\n",
    "df_stats = pd.read_csv('../data/model_stats.csv')\n",
    "\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's explore our model stats via sorting/visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer + Logreg/MNB stack</td>\n",
       "      <td>10.692293</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.941424</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.950041</td>\n",
       "      <td>0.940795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer + tuned LogisticRegression</td>\n",
       "      <td>4.055915</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.938014</td>\n",
       "      <td>0.918474</td>\n",
       "      <td>0.955704</td>\n",
       "      <td>0.936719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer + LogReg/MNB/Grad Stack</td>\n",
       "      <td>15.633441</td>\n",
       "      <td>0.952782</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.936878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer + Logreg/Grad stack</td>\n",
       "      <td>14.953498</td>\n",
       "      <td>0.957665</td>\n",
       "      <td>0.937212</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>0.950352</td>\n",
       "      <td>0.936214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer + LogisticRegression</td>\n",
       "      <td>1.495226</td>\n",
       "      <td>0.956996</td>\n",
       "      <td>0.936810</td>\n",
       "      <td>0.917671</td>\n",
       "      <td>0.954071</td>\n",
       "      <td>0.935517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TfidfVectorizer + MNB/Grad stack</td>\n",
       "      <td>4.415673</td>\n",
       "      <td>0.945693</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.922892</td>\n",
       "      <td>0.939877</td>\n",
       "      <td>0.931307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + LogisticRegression</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.975254</td>\n",
       "      <td>0.931795</td>\n",
       "      <td>0.912048</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.930356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF + MultinomialNB</td>\n",
       "      <td>1.296156</td>\n",
       "      <td>0.941413</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.929810</td>\n",
       "      <td>0.927752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TfidfVectorizer + GradientBoost</td>\n",
       "      <td>2.421923</td>\n",
       "      <td>0.933119</td>\n",
       "      <td>0.924574</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.962380</td>\n",
       "      <td>0.921273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TfidfVectorizer + ExtraTrees</td>\n",
       "      <td>1.858499</td>\n",
       "      <td>0.921750</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.910387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer + Bagging</td>\n",
       "      <td>28.872257</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.906319</td>\n",
       "      <td>0.884739</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.904166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer + Random Forest</td>\n",
       "      <td>2.628158</td>\n",
       "      <td>0.914460</td>\n",
       "      <td>0.903310</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.960128</td>\n",
       "      <td>0.896832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer + AdaBoost</td>\n",
       "      <td>7.388280</td>\n",
       "      <td>0.900482</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.952206</td>\n",
       "      <td>0.888127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model   fit_time  train_acc  \\\n",
       "12          TfidfVectorizer + Logreg/MNB stack  10.692293   0.957263   \n",
       "2   TfidfVectorizer + tuned LogisticRegression   4.055915   0.955257   \n",
       "9      TfidfVectorizer + LogReg/MNB/Grad Stack  15.633441   0.952782   \n",
       "11         TfidfVectorizer + Logreg/Grad stack  14.953498   0.957665   \n",
       "1         TfidfVectorizer + LogisticRegression   1.495226   0.956996   \n",
       "10            TfidfVectorizer + MNB/Grad stack   4.415673   0.945693   \n",
       "0         CountVectorizer + LogisticRegression   0.874005   0.975254   \n",
       "3                        TFIDF + MultinomialNB   1.296156   0.941413   \n",
       "8              TfidfVectorizer + GradientBoost   2.421923   0.933119   \n",
       "6                 TfidfVectorizer + ExtraTrees   1.858499   0.921750   \n",
       "4                    TfidfVectorizer + Bagging  28.872257   0.992376   \n",
       "5              TfidfVectorizer + Random Forest   2.628158   0.914460   \n",
       "7                   TfidfVectorizer + AdaBoost   7.388280   0.900482   \n",
       "\n",
       "    test_acc    recall  precision        f1  \n",
       "12  0.941424  0.931727   0.950041  0.940795  \n",
       "2   0.938014  0.918474   0.955704  0.936719  \n",
       "9   0.937613  0.926908   0.947066  0.936878  \n",
       "11  0.937212  0.922490   0.950352  0.936214  \n",
       "1   0.936810  0.917671   0.954071  0.935517  \n",
       "10  0.931996  0.922892   0.939877  0.931307  \n",
       "0   0.931795  0.912048   0.949415  0.930356  \n",
       "3   0.927984  0.925703   0.929810  0.927752  \n",
       "8   0.924574  0.883534   0.962380  0.921273  \n",
       "6   0.911735  0.897590   0.923554  0.910387  \n",
       "4   0.906319  0.884739   0.924465  0.904166  \n",
       "5   0.903310  0.841365   0.960128  0.896832  \n",
       "7   0.895286  0.832129   0.952206  0.888127  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Who's got the best test set accuracy?\n",
    "\n",
    "df_stats.sort_values(by = 'test_acc', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- As we saw as we went through our models, TFIDF + LogisticRegression and stacks using permutations of LogisticRegression, MultinomialNB, and GradientBoost are our winners here.\n",
    "- Interesting that a CountVectorizer-based model made it into our top 10 as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer + AdaBoost</td>\n",
       "      <td>7.388280</td>\n",
       "      <td>0.900482</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.952206</td>\n",
       "      <td>0.888127</td>\n",
       "      <td>0.005196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TfidfVectorizer + GradientBoost</td>\n",
       "      <td>2.421923</td>\n",
       "      <td>0.933119</td>\n",
       "      <td>0.924574</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.962380</td>\n",
       "      <td>0.921273</td>\n",
       "      <td>0.008546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TfidfVectorizer + ExtraTrees</td>\n",
       "      <td>1.858499</td>\n",
       "      <td>0.921750</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.910387</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer + Random Forest</td>\n",
       "      <td>2.628158</td>\n",
       "      <td>0.914460</td>\n",
       "      <td>0.903310</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.960128</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>0.011150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF + MultinomialNB</td>\n",
       "      <td>1.296156</td>\n",
       "      <td>0.941413</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.929810</td>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.013429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TfidfVectorizer + MNB/Grad stack</td>\n",
       "      <td>4.415673</td>\n",
       "      <td>0.945693</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.922892</td>\n",
       "      <td>0.939877</td>\n",
       "      <td>0.931307</td>\n",
       "      <td>0.013697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer + LogReg/MNB/Grad Stack</td>\n",
       "      <td>15.633441</td>\n",
       "      <td>0.952782</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.936878</td>\n",
       "      <td>0.015169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer + Logreg/MNB stack</td>\n",
       "      <td>10.692293</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.941424</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.950041</td>\n",
       "      <td>0.940795</td>\n",
       "      <td>0.015839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer + tuned LogisticRegression</td>\n",
       "      <td>4.055915</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.938014</td>\n",
       "      <td>0.918474</td>\n",
       "      <td>0.955704</td>\n",
       "      <td>0.936719</td>\n",
       "      <td>0.017243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer + LogisticRegression</td>\n",
       "      <td>1.495226</td>\n",
       "      <td>0.956996</td>\n",
       "      <td>0.936810</td>\n",
       "      <td>0.917671</td>\n",
       "      <td>0.954071</td>\n",
       "      <td>0.935517</td>\n",
       "      <td>0.020185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer + Logreg/Grad stack</td>\n",
       "      <td>14.953498</td>\n",
       "      <td>0.957665</td>\n",
       "      <td>0.937212</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>0.950352</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>0.020453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + LogisticRegression</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.975254</td>\n",
       "      <td>0.931795</td>\n",
       "      <td>0.912048</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.930356</td>\n",
       "      <td>0.043459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer + Bagging</td>\n",
       "      <td>28.872257</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.906319</td>\n",
       "      <td>0.884739</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.904166</td>\n",
       "      <td>0.086057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model   fit_time  train_acc  \\\n",
       "7                   TfidfVectorizer + AdaBoost   7.388280   0.900482   \n",
       "8              TfidfVectorizer + GradientBoost   2.421923   0.933119   \n",
       "6                 TfidfVectorizer + ExtraTrees   1.858499   0.921750   \n",
       "5              TfidfVectorizer + Random Forest   2.628158   0.914460   \n",
       "3                        TFIDF + MultinomialNB   1.296156   0.941413   \n",
       "10            TfidfVectorizer + MNB/Grad stack   4.415673   0.945693   \n",
       "9      TfidfVectorizer + LogReg/MNB/Grad Stack  15.633441   0.952782   \n",
       "12          TfidfVectorizer + Logreg/MNB stack  10.692293   0.957263   \n",
       "2   TfidfVectorizer + tuned LogisticRegression   4.055915   0.955257   \n",
       "1         TfidfVectorizer + LogisticRegression   1.495226   0.956996   \n",
       "11         TfidfVectorizer + Logreg/Grad stack  14.953498   0.957665   \n",
       "0         CountVectorizer + LogisticRegression   0.874005   0.975254   \n",
       "4                    TfidfVectorizer + Bagging  28.872257   0.992376   \n",
       "\n",
       "    test_acc    recall  precision        f1  accuracy_delta  \n",
       "7   0.895286  0.832129   0.952206  0.888127        0.005196  \n",
       "8   0.924574  0.883534   0.962380  0.921273        0.008546  \n",
       "6   0.911735  0.897590   0.923554  0.910387        0.010014  \n",
       "5   0.903310  0.841365   0.960128  0.896832        0.011150  \n",
       "3   0.927984  0.925703   0.929810  0.927752        0.013429  \n",
       "10  0.931996  0.922892   0.939877  0.931307        0.013697  \n",
       "9   0.937613  0.926908   0.947066  0.936878        0.015169  \n",
       "12  0.941424  0.931727   0.950041  0.940795        0.015839  \n",
       "2   0.938014  0.918474   0.955704  0.936719        0.017243  \n",
       "1   0.936810  0.917671   0.954071  0.935517        0.020185  \n",
       "11  0.937212  0.922490   0.950352  0.936214        0.020453  \n",
       "0   0.931795  0.912048   0.949415  0.930356        0.043459  \n",
       "4   0.906319  0.884739   0.924465  0.904166        0.086057  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How about the delta between train and test?\n",
    "\n",
    "df_stats['accuracy_delta'] = df_stats['train_acc'] - df_stats['test_acc']\n",
    "\n",
    "df_stats.sort_values(by = 'accuracy_delta')\n",
    "\n",
    "# No negative values for delta, so we're OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- The boosting models closed the variance gap the best, followed by the tree ensembles, single tree, MultinomialNB and some of our stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + LogisticRegression</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.975254</td>\n",
       "      <td>0.931795</td>\n",
       "      <td>0.912048</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.930356</td>\n",
       "      <td>0.043459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF + MultinomialNB</td>\n",
       "      <td>1.296156</td>\n",
       "      <td>0.941413</td>\n",
       "      <td>0.927984</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.929810</td>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.013429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer + LogisticRegression</td>\n",
       "      <td>1.495226</td>\n",
       "      <td>0.956996</td>\n",
       "      <td>0.936810</td>\n",
       "      <td>0.917671</td>\n",
       "      <td>0.954071</td>\n",
       "      <td>0.935517</td>\n",
       "      <td>0.020185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TfidfVectorizer + ExtraTrees</td>\n",
       "      <td>1.858499</td>\n",
       "      <td>0.921750</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.897590</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.910387</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TfidfVectorizer + GradientBoost</td>\n",
       "      <td>2.421923</td>\n",
       "      <td>0.933119</td>\n",
       "      <td>0.924574</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.962380</td>\n",
       "      <td>0.921273</td>\n",
       "      <td>0.008546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfidfVectorizer + Random Forest</td>\n",
       "      <td>2.628158</td>\n",
       "      <td>0.914460</td>\n",
       "      <td>0.903310</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.960128</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>0.011150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer + tuned LogisticRegression</td>\n",
       "      <td>4.055915</td>\n",
       "      <td>0.955257</td>\n",
       "      <td>0.938014</td>\n",
       "      <td>0.918474</td>\n",
       "      <td>0.955704</td>\n",
       "      <td>0.936719</td>\n",
       "      <td>0.017243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TfidfVectorizer + MNB/Grad stack</td>\n",
       "      <td>4.415673</td>\n",
       "      <td>0.945693</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.922892</td>\n",
       "      <td>0.939877</td>\n",
       "      <td>0.931307</td>\n",
       "      <td>0.013697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TfidfVectorizer + AdaBoost</td>\n",
       "      <td>7.388280</td>\n",
       "      <td>0.900482</td>\n",
       "      <td>0.895286</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.952206</td>\n",
       "      <td>0.888127</td>\n",
       "      <td>0.005196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer + Logreg/MNB stack</td>\n",
       "      <td>10.692293</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.941424</td>\n",
       "      <td>0.931727</td>\n",
       "      <td>0.950041</td>\n",
       "      <td>0.940795</td>\n",
       "      <td>0.015839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TfidfVectorizer + Logreg/Grad stack</td>\n",
       "      <td>14.953498</td>\n",
       "      <td>0.957665</td>\n",
       "      <td>0.937212</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>0.950352</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>0.020453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TfidfVectorizer + LogReg/MNB/Grad Stack</td>\n",
       "      <td>15.633441</td>\n",
       "      <td>0.952782</td>\n",
       "      <td>0.937613</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.936878</td>\n",
       "      <td>0.015169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer + Bagging</td>\n",
       "      <td>28.872257</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.906319</td>\n",
       "      <td>0.884739</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.904166</td>\n",
       "      <td>0.086057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model   fit_time  train_acc  \\\n",
       "0         CountVectorizer + LogisticRegression   0.874005   0.975254   \n",
       "3                        TFIDF + MultinomialNB   1.296156   0.941413   \n",
       "1         TfidfVectorizer + LogisticRegression   1.495226   0.956996   \n",
       "6                 TfidfVectorizer + ExtraTrees   1.858499   0.921750   \n",
       "8              TfidfVectorizer + GradientBoost   2.421923   0.933119   \n",
       "5              TfidfVectorizer + Random Forest   2.628158   0.914460   \n",
       "2   TfidfVectorizer + tuned LogisticRegression   4.055915   0.955257   \n",
       "10            TfidfVectorizer + MNB/Grad stack   4.415673   0.945693   \n",
       "7                   TfidfVectorizer + AdaBoost   7.388280   0.900482   \n",
       "12          TfidfVectorizer + Logreg/MNB stack  10.692293   0.957263   \n",
       "11         TfidfVectorizer + Logreg/Grad stack  14.953498   0.957665   \n",
       "9      TfidfVectorizer + LogReg/MNB/Grad Stack  15.633441   0.952782   \n",
       "4                    TfidfVectorizer + Bagging  28.872257   0.992376   \n",
       "\n",
       "    test_acc    recall  precision        f1  accuracy_delta  \n",
       "0   0.931795  0.912048   0.949415  0.930356        0.043459  \n",
       "3   0.927984  0.925703   0.929810  0.927752        0.013429  \n",
       "1   0.936810  0.917671   0.954071  0.935517        0.020185  \n",
       "6   0.911735  0.897590   0.923554  0.910387        0.010014  \n",
       "8   0.924574  0.883534   0.962380  0.921273        0.008546  \n",
       "5   0.903310  0.841365   0.960128  0.896832        0.011150  \n",
       "2   0.938014  0.918474   0.955704  0.936719        0.017243  \n",
       "10  0.931996  0.922892   0.939877  0.931307        0.013697  \n",
       "7   0.895286  0.832129   0.952206  0.888127        0.005196  \n",
       "12  0.941424  0.931727   0.950041  0.940795        0.015839  \n",
       "11  0.937212  0.922490   0.950352  0.936214        0.020453  \n",
       "9   0.937613  0.926908   0.947066  0.936878        0.015169  \n",
       "4   0.906319  0.884739   0.924465  0.904166        0.086057  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How about speed of fit?\n",
    "\n",
    "df_stats.sort_values(by = 'fit_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- As we might expect, some of the simpler models fit more quickly.\n",
    "- Our stacks don't show up so well on this ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to find the \"sweet spot\" that maximizes test accuracy and minimizes the accuracy delta (overfitting) and fit time.\n",
    "- So we need an optimization technique to figure out which model to use!\n",
    "- Proposed solution:\n",
    "    - Create 2 new columns for inverse of accuracy delta and inverse of fit time\n",
    "    - Scale those 2 columns, along with test accuracy, so they have the same magnitude\n",
    "    - Create (somewhat arbitrary) weighting to prioritize the 3 variables\n",
    "    - Choose the model with the maximum sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>inv_accuracy_delta</th>\n",
       "      <th>inv_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.931795</td>\n",
       "      <td>23.010320</td>\n",
       "      <td>1.144158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.936810</td>\n",
       "      <td>49.541031</td>\n",
       "      <td>0.668795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938014</td>\n",
       "      <td>57.995289</td>\n",
       "      <td>0.246553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.927984</td>\n",
       "      <td>74.468103</td>\n",
       "      <td>0.771512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.906319</td>\n",
       "      <td>11.620253</td>\n",
       "      <td>0.034635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.903310</td>\n",
       "      <td>89.688719</td>\n",
       "      <td>0.380495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.911735</td>\n",
       "      <td>99.856276</td>\n",
       "      <td>0.538069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.895286</td>\n",
       "      <td>192.467464</td>\n",
       "      <td>0.135349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.924574</td>\n",
       "      <td>117.019367</td>\n",
       "      <td>0.412895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.937613</td>\n",
       "      <td>65.922194</td>\n",
       "      <td>0.063965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.931996</td>\n",
       "      <td>73.009243</td>\n",
       "      <td>0.226466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.937212</td>\n",
       "      <td>48.892842</td>\n",
       "      <td>0.066874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.941424</td>\n",
       "      <td>63.135420</td>\n",
       "      <td>0.093525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_acc  inv_accuracy_delta  inv_fit_time\n",
       "0   0.931795           23.010320      1.144158\n",
       "1   0.936810           49.541031      0.668795\n",
       "2   0.938014           57.995289      0.246553\n",
       "3   0.927984           74.468103      0.771512\n",
       "4   0.906319           11.620253      0.034635\n",
       "5   0.903310           89.688719      0.380495\n",
       "6   0.911735           99.856276      0.538069\n",
       "7   0.895286          192.467464      0.135349\n",
       "8   0.924574          117.019367      0.412895\n",
       "9   0.937613           65.922194      0.063965\n",
       "10  0.931996           73.009243      0.226466\n",
       "11  0.937212           48.892842      0.066874\n",
       "12  0.941424           63.135420      0.093525"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our inverse columns\n",
    "df_stats['inv_accuracy_delta'] = df_stats['accuracy_delta'] ** -1\n",
    "df_stats['inv_fit_time'] = df_stats['fit_time'] ** -1\n",
    "\n",
    "# New dataset focusing on just what we need\n",
    "df_opt_stats = df_stats[['test_acc', 'inv_accuracy_delta', 'inv_fit_time']]\n",
    "df_opt_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>inv_accuracy_delta</th>\n",
       "      <th>inv_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.462207</td>\n",
       "      <td>-1.171828</td>\n",
       "      <td>2.412914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799774</td>\n",
       "      <td>-0.566334</td>\n",
       "      <td>0.935212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.880790</td>\n",
       "      <td>-0.373388</td>\n",
       "      <td>-0.377356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.205656</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>1.254516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.252633</td>\n",
       "      <td>-1.431776</td>\n",
       "      <td>-1.036119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.455173</td>\n",
       "      <td>0.349932</td>\n",
       "      <td>0.039009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.888061</td>\n",
       "      <td>0.581980</td>\n",
       "      <td>0.528840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.995280</td>\n",
       "      <td>2.695587</td>\n",
       "      <td>-0.723042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.023889</td>\n",
       "      <td>0.973682</td>\n",
       "      <td>0.139729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.853785</td>\n",
       "      <td>-0.192477</td>\n",
       "      <td>-0.944945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.475710</td>\n",
       "      <td>-0.030734</td>\n",
       "      <td>-0.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.826779</td>\n",
       "      <td>-0.581127</td>\n",
       "      <td>-0.935903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.110336</td>\n",
       "      <td>-0.256078</td>\n",
       "      <td>-0.853056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_acc  inv_accuracy_delta  inv_fit_time\n",
       "0   0.462207           -1.171828      2.412914\n",
       "1   0.799774           -0.566334      0.935212\n",
       "2   0.880790           -0.373388     -0.377356\n",
       "3   0.205656            0.002561      1.254516\n",
       "4  -1.252633           -1.431776     -1.036119\n",
       "5  -1.455173            0.349932      0.039009\n",
       "6  -0.888061            0.581980      0.528840\n",
       "7  -1.995280            2.695587     -0.723042\n",
       "8  -0.023889            0.973682      0.139729\n",
       "9   0.853785           -0.192477     -0.944945\n",
       "10  0.475710           -0.030734     -0.439800\n",
       "11  0.826779           -0.581127     -0.935903\n",
       "12  1.110336           -0.256078     -0.853056"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale\n",
    "\n",
    "ss = StandardScaler()\n",
    "df_opt_stats_sc = pd.DataFrame(ss.fit_transform(df_opt_stats), columns = df_opt_stats.columns)\n",
    "\n",
    "df_opt_stats_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need some weights\n",
    "# These are pretty arbitrary!\n",
    "# So we're saying test accuracy is 2x as important as closing the overfitting gap\n",
    "# And that's 2x as important as how long it takes to fit\n",
    "\n",
    "weight_test_acc = 4\n",
    "weight_inv_accuracy_delta = 2\n",
    "weight_inv_fit_time = 1\n",
    "\n",
    "df_opt_stats_sc['overall_score'] = weight_test_acc * df_opt_stats_sc['test_acc'] + \\\n",
    "    weight_inv_accuracy_delta * df_opt_stats_sc['inv_accuracy_delta'] + \\\n",
    "        weight_inv_fit_time * df_opt_stats_sc['inv_fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put the model names back in\n",
    "\n",
    "df_opt_stats_sc['model'] = df_stats['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_acc</th>\n",
       "      <th>inv_accuracy_delta</th>\n",
       "      <th>inv_fit_time</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.110336</td>\n",
       "      <td>-0.256078</td>\n",
       "      <td>-0.853056</td>\n",
       "      <td>3.076131</td>\n",
       "      <td>TfidfVectorizer + Logreg/MNB stack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799774</td>\n",
       "      <td>-0.566334</td>\n",
       "      <td>0.935212</td>\n",
       "      <td>3.001640</td>\n",
       "      <td>TfidfVectorizer + LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.880790</td>\n",
       "      <td>-0.373388</td>\n",
       "      <td>-0.377356</td>\n",
       "      <td>2.399028</td>\n",
       "      <td>TfidfVectorizer + tuned LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.853785</td>\n",
       "      <td>-0.192477</td>\n",
       "      <td>-0.944945</td>\n",
       "      <td>2.085240</td>\n",
       "      <td>TfidfVectorizer + LogReg/MNB/Grad Stack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.205656</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>1.254516</td>\n",
       "      <td>2.082263</td>\n",
       "      <td>TFIDF + MultinomialNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.023889</td>\n",
       "      <td>0.973682</td>\n",
       "      <td>0.139729</td>\n",
       "      <td>1.991535</td>\n",
       "      <td>TfidfVectorizer + GradientBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.462207</td>\n",
       "      <td>-1.171828</td>\n",
       "      <td>2.412914</td>\n",
       "      <td>1.918086</td>\n",
       "      <td>CountVectorizer + LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.475710</td>\n",
       "      <td>-0.030734</td>\n",
       "      <td>-0.439800</td>\n",
       "      <td>1.401572</td>\n",
       "      <td>TfidfVectorizer + MNB/Grad stack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.826779</td>\n",
       "      <td>-0.581127</td>\n",
       "      <td>-0.935903</td>\n",
       "      <td>1.208960</td>\n",
       "      <td>TfidfVectorizer + Logreg/Grad stack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.888061</td>\n",
       "      <td>0.581980</td>\n",
       "      <td>0.528840</td>\n",
       "      <td>-1.859444</td>\n",
       "      <td>TfidfVectorizer + ExtraTrees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.995280</td>\n",
       "      <td>2.695587</td>\n",
       "      <td>-0.723042</td>\n",
       "      <td>-3.312988</td>\n",
       "      <td>TfidfVectorizer + AdaBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.455173</td>\n",
       "      <td>0.349932</td>\n",
       "      <td>0.039009</td>\n",
       "      <td>-5.081820</td>\n",
       "      <td>TfidfVectorizer + Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.252633</td>\n",
       "      <td>-1.431776</td>\n",
       "      <td>-1.036119</td>\n",
       "      <td>-8.910204</td>\n",
       "      <td>TfidfVectorizer + Bagging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_acc  inv_accuracy_delta  inv_fit_time  overall_score  \\\n",
       "12  1.110336           -0.256078     -0.853056       3.076131   \n",
       "1   0.799774           -0.566334      0.935212       3.001640   \n",
       "2   0.880790           -0.373388     -0.377356       2.399028   \n",
       "9   0.853785           -0.192477     -0.944945       2.085240   \n",
       "3   0.205656            0.002561      1.254516       2.082263   \n",
       "8  -0.023889            0.973682      0.139729       1.991535   \n",
       "0   0.462207           -1.171828      2.412914       1.918086   \n",
       "10  0.475710           -0.030734     -0.439800       1.401572   \n",
       "11  0.826779           -0.581127     -0.935903       1.208960   \n",
       "6  -0.888061            0.581980      0.528840      -1.859444   \n",
       "7  -1.995280            2.695587     -0.723042      -3.312988   \n",
       "5  -1.455173            0.349932      0.039009      -5.081820   \n",
       "4  -1.252633           -1.431776     -1.036119      -8.910204   \n",
       "\n",
       "                                         model  \n",
       "12          TfidfVectorizer + Logreg/MNB stack  \n",
       "1         TfidfVectorizer + LogisticRegression  \n",
       "2   TfidfVectorizer + tuned LogisticRegression  \n",
       "9      TfidfVectorizer + LogReg/MNB/Grad Stack  \n",
       "3                        TFIDF + MultinomialNB  \n",
       "8              TfidfVectorizer + GradientBoost  \n",
       "0         CountVectorizer + LogisticRegression  \n",
       "10            TfidfVectorizer + MNB/Grad stack  \n",
       "11         TfidfVectorizer + Logreg/Grad stack  \n",
       "6                 TfidfVectorizer + ExtraTrees  \n",
       "7                   TfidfVectorizer + AdaBoost  \n",
       "5              TfidfVectorizer + Random Forest  \n",
       "4                    TfidfVectorizer + Bagging  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moment of truth...\n",
    "\n",
    "df_opt_stats_sc.sort_values(by = 'overall_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- As we expected, our top candidates are TFIDF + LogisticRegression and related stacks.\n",
    "- Let's go with #1! TFIDF + Logreg/MNB stack it is. It was our very last model!\n",
    "- We will now recreate that model and fit it to our clean data, double-check the scores, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a slightly modified version of the evaluation function used in the previous notebook\n",
    "\n",
    "def evaluation(model, model_desc, X_train, y_train, X_test, y_test, normalize = 'pred'):\n",
    "    '''\n",
    "    Function to print and acquire some quick model stats.\n",
    "\n",
    "    Parameters:\n",
    "        model: the model\n",
    "        model: description of the model\n",
    "        X_train, y_train: Training features and target\n",
    "        X_test, y_test: Testing features and target\n",
    "        normalize: normalization for ConfusionMatrixDisplay\n",
    "    Returns:\n",
    "        Dataframe containing the model, fit time, training accuracy, testing accuracy, recall, precision, and F1\n",
    "    '''\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    stop_time = time.time()\n",
    "    fit_time = stop_time - start_time\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Blues',\n",
    "    normalize=normalize)\n",
    "\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    recall = recall_score(y_test, preds, pos_label = 1)\n",
    "    precision = precision_score(y_test, preds, pos_label = 1)\n",
    "    f1 = f1_score(y_test, preds, pos_label = 1)\n",
    "    \n",
    "    print(f'fit time: {fit_time}')\n",
    "    print(f'training set accuracy: {train_acc}')\n",
    "    print(f'test set accuracy: {test_acc}')\n",
    "    print(f'recall: {recall}')\n",
    "    print(f'precision: {precision}')\n",
    "    print(f'f1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redoing our couple of transformations on our clean posts to recreate our X and y and train-test split\n",
    "df = pd.read_csv('../data/clean_posts.csv')\n",
    "\n",
    "# We're focusing on just 2 classes, so let's drop the others\n",
    "df = df[(df['subreddit'] == 'classicalmusic') | (df['subreddit'] == 'jazz')]\n",
    "\n",
    "# let's encode our classes for efficiency\n",
    "df['subreddit_code'] = df['subreddit'].astype('category').cat.codes\n",
    "\n",
    "X = df['all_text']\n",
    "y = df['subreddit_code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time: 10.132207870483398\n",
      "training set accuracy: 0.9572632423756019\n",
      "test set accuracy: 0.9414242728184554\n",
      "recall: 0.9317269076305221\n",
      "precision: 0.95004095004095\n",
      "f1 score: 0.9407948094079481\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEGCAYAAAAE8QIHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAa1ElEQVR4nO3de5xVdb3/8dd7ZrgpdxiugqKZSF4R8ZZGmoWeTgp5ymtmeczfCa1+lvo7dfSkD4+mWWZoZGZqWaRHTEuUvIYmPuSiomAogshFBFQIEcWZ+fz+2JtpzzTM7AWzZ69Z83762A/3Wuu7v+uz2Q8+fL9rfb/fpYjAzCwrKsodgJlZa3JSM7NMcVIzs0xxUjOzTHFSM7NMqSp3AIVU1S3UpWe5w7AEDhw5rNwhWALLlr3GunXrtCN1VPbcNaJmc1FlY/PaGRExfkfOl1S6klqXnnQZdVq5w7AE/jrrR+UOwRI44pAxO1xH1LxPl5EnF1X2/Wd/2n+HT5hQqpKambUDArRDjb2SclIzs+SU3svxTmpmlpxbamaWHYKKynIHsU1OamaWjHD308yyRO5+mlnGuKVmZpnilpqZZYfcUjOzDBG++2lmWeKWmpllTYWvqZlZVnicmpllju9+mll2eJqUmWWNu59mlhnyNCkzyxq31MwsU9xSM7Ps8OBbM8sST5Mys2xxS83MssbX1MwsU9xSM7NMcUvNzDJDvqZmZhmjCic1M8sIAXL308wyQ/lXSjmpmVlCckvNzLIlzUktvVf7zCy1Kioqinq1RNJ4SYskLZZ0cRPHe0n6o6TnJS2QdFaLsW3ndzKzjkoJXs1VI1UCNwDHAaOAUySNalTs68DCiNgfGAdcK6lzc/U6qZlZIspfUyvm1YKxwOKIWBIRW4CpwAmNygTQQ7nKugNvAzXNVepramaWWCtdUxsKLC/YXgEc0qjMZOA+YBXQA/hiRNQ1V6lbamaWWIKWWn9Jcwpe5xRW00TV0Wj7M8BzwBDgAGCypJ7NxeaWmpkllqClti4ixmzj2ApgWMH2LuRaZIXOAq6KiAAWS1oKjASe2dYJ3VIzs2QEqlBRrxbMBvaUNCJ/8f9kcl3NQq8DxwBIGgjsBSxprlK31MwsEbXS4NuIqJE0CZgBVAK3RMQCSefmj08BLgdulfQCue7qRRGxrrl6ndTMLLHWGnwbEdOB6Y32TSl4vwr4dJI6ndTMLLn0TihwUjOzhJTuaVJOamaWmJOamWWGUFHzOsvFSc3MkktvQ81JzcwS8jU1M8saJzUzyxQnNTPLlCKmQJWNk9oOOObQkVz5zROprKzg1/c9zXW/frTB8V49ujH5uyczYmg/3t9Sw3lXTOWlJavp0rmK+382iS6dqqisrOC+x57nqptnlOlbdCwPP7WQ/3ft/1JbV8cZJxzOt77ccLB6RHDxtf/LQ39dQLeunbnx0jPYf2RuzvV+n7uE7jt1obKigqqqCh67/aJyfIWyK3KttLIpaVKTNB74Cbl5XTdHxFWlPF9bqqgQ11wwkQnfmMKqNRt49JZv8cATC1j02pv1ZS4481O88PJKzrj4V+y56wCu+fZETjxvCh9sqeGESTeyafMWqioreODn5/HwrL8xZ8GyMn6j7KutreM7V9/JPZMnMWRgb44+8xqOO2pfRu4+uL7MQ08t5NXX1zJ32qXMefE1LrhqKg/f+p3643+c8g369e5ejvBTJc1JrWSDTYpcqrfdOmjUcJasWMeyVW/zYU0t0x5+luOP2qdBmb12G8jMOa8A8MqyNQwf1JfqPrm/EJs2bwGgU1Ulnaoqya2sYqU0d8Fr7D6sP7vt0p/OnaqYeOxopv9lfoMy0/8yn5P/ZSySOHjfEWzYuJnV6zaUKeL0aqWVb0uilCPoilmqt90aXN2LlWvW12+vWrOewdW9GpR5cfEqPjtuXwBGjxrOsEF9GDKgN5Br6c287QJenn4Zjz/zMnMXvt5WoXdYb6zdwNCBfeq3hwzswxtrNzQqs75hmQG9eSP/O0ti4qTJjDvjB9w67ck2iTm1WuEZBaVSyu5nMUv1kl8JM7caZuceJQyndTX1r1Dj1tZ1tz/Cld+awMzbLmDhq28w/+WV1NbmViKuqwuOOvNaenbvym+u+gp77z6Il5asbpPYO6qmWsONf8amGsxbf+sHb/4Wg6t7s/btjUyYNJk9dxvEEaM/UopQUy/N3c9SJrViluolIm4CbgKo2Hlgu+mDrVqznqH5Vhfk/kVfve7vDcpsfO8DJl0xtX77+WnfY9mqtxqU+fu77/PkvMUcc+hIJ7USGzKgNyvffKd+e9Wb7zCof6/my6xZz6B8C3xwdW8Aqvv24LPj9mPegtc6ZFKTcj2NtCpl97OYpXrbrXkvLWePYdUMH9yXTlWVTPzUgTzwxIsNyvTs3pVOVZUAfOlzh/LUc6+y8b0P6Nd7Z3p27wpA1y6dGHfwR3ll2Zo2/w4dzehRu/Lq62tZtnIdWz6sYdpD8zjuqP0alDnuqH2Zev8zRASzX1hKz+7dGNS/F5s2f8DGTe8DsGnzBzz69N/Ye48h5fgaKdBqT5MqiVK21OqX6gVWkluq99QSnq9N1dbWceG107j7unOorKjgjj89w9+WvslZEw4D4Ff3zGKv3Qbys0tOpbaujkVL3+S8//k9AIP69eTGS06hsqKCCol7Hn2eGX9dWM6v0yFUVVVy9YVf4PPn30BtbXDa5w5l7z0Gc8vdTwDwlc8fyaeP+BgP/XUBoyd8n25dO3HDJacDsPatjZx+4S8AqK2p5fPjx/CpwzNz3yuxFPc+USnvukk6HriOfyzVe0Vz5St2HhhdRp1Wsnis9b0z60flDsESOOKQMcydO2eHUlLXQR+NXc/8aVFlX756/NxmHrxSEiUdp9bUUr1m1s4p3S01zygws0REum8UOKmZWWJOamaWHe5+mlmWiI47+NbMMqkDr9JhZtmU4pzmpGZmCaV8mpSTmpkl4mtqZpY5Kc5pTmpmlpxbamaWKSnOaU5qZpaQH2ZsZlki5LufZpYtKW6oOamZWXLufppZdnhCu5lliQffmlnmOKmZWab47qeZZYevqZlZlijl66mV8mHGZpZRUnGvluvReEmLJC2WdPE2yoyT9JykBZL+0lKdbqmZWWIVrdBSk1QJ3AAcC6wAZku6LyIWFpTpDdwIjI+I1yUNaKleJzUzS0Stt0jkWGBxRCzJ1aupwAnAwoIypwLTIuJ1gIhY01Kl7n6aWWIVKu4F9Jc0p+B1TkE1Q4HlBdsr8vsKfRToI+lxSXMlfaml2NxSM7PEEtwoWBcRY7ZVTRP7otF2FXAQcAzQDZgl6emIeHlbJ9xmUpP00yZO8I8zR5y/rWNmlm2tdPNzBTCsYHsXYFUTZdZFxCZgk6SZwP5A8qQGzNnOQM0sw0RuWEcrmA3sKWkEsBI4mdw1tEL3ApMlVQGdgUOAHzdX6TaTWkTcVrgtaed8tjSzDq417hNERI2kScAMoBK4JSIWSDo3f3xKRLwk6UFgPlAH3BwRLzZXb4vX1CQdBvwS6A4Ml7Q/8LWI+I8d+0pm1i6p9RaJjIjpwPRG+6Y02r4GuKbYOou5+3kd8BngrfwJngeOKvYEZpYtIjdOrZhXORR19zMilje621FbmnDMrD1I8SypopLackmHAyGpM3A+8FJpwzKzNGvvcz/PBb5OblDcSuCA/LaZdUDFzvssV95rsaUWEeuA09ogFjNrJyrbc0tN0u6S/ihpraQ1ku6VtHtbBGdm6SSpqFc5FNP9/C1wJzAYGALcBfyulEGZWXrl7n4WPfezzRWT1BQRv46ImvzrNzQzfcrMMq7IVlq5WmrNzf3sm3/7WH7xtqnkktkXgfvbIDYzS6kUX1Jr9kbBXHJJbGv4Xys4FsDlpQrKzNItzUM6mpv7OaItAzGz9kFAZXt/mpSkfYBRQNet+yLi9lIFZWbplt6UVtyE9kuBceSS2nTgOOBJwEnNrAOSWucZBaVSzN3Pk8itOrk6Is4it0Bbl5JGZWap1q5nFACbI6JOUo2knsAawINvzTqwdnmjoMCc/GOqfkHujui7wDOlDMrM0i3FOa2ouZ9bF4Ockl+BsmdEzC9tWGaWVpLa591PSaObOxYR80oTkpmlXXvtfl7bzLEAjm7lWDhg5DBmPvnD1q7WSqjPwZPKHYIl8MGi11ulnjQ/MLi5wbefbMtAzKx9EO23pWZm1qQUX1JzUjOzZKQMTJMyMyuU4pxW1Mq3knS6pEvy28MljS19aGaWVmmeUVDMTYwbgcOAU/LbG4EbShaRmaVaFp77eUhEjJb0LEBEvJN/VJ6ZdVDtckhHgQ8lVZJfwltSNVBX0qjMLNVSPKKjqKR2PXAPMEDSFeRW7fheSaMys9Rqt9OktoqIOyTNJbf8kIATI8JPaDfrwFKc04paJHI48B7wx8J9EdE68y3MrF3ZeqMgrYrpft7PPx7A0hUYASwCPlbCuMwsxVKc04rqfu5buJ1fveNr2yhuZllXxgcVFyPxjIKImCfp4FIEY2btg1L86JVirqn934LNCmA0sLZkEZlZqgmoSvFAtWJaaj0K3teQu8Z2d2nCMbP2oN0uPZQfdNs9Ir7TRvGYWcrl7n6WO4pta24576qIqGluWW8z64DKOFm9GM211J4hd/3sOUn3AXcBm7YejIhpJY7NzFKqvY9T6wu8Re6ZBFvHqwXgpGbWAQmoTPGNguZCG5C/8/ki8EL+/wvy/3+xDWIzs1QSFUW+WqxJGi9pkaTFki5uptzBkmolndRSnc211CqB7tBkZNFitGaWSbkHr7RCPbkbkTcAxwIrgNmS7ouIhU2U+wEwo5h6m0tqb0TEZdsZr5llVevNKBgLLI6IJQCSpgInAAsblTuP3DCyogb9N5fU0nsl0MzKKsGNgv6S5hRs3xQRN+XfDwWWFxxbARxS+GFJQ4EJ5K7p73BSO6aYCsysY0nY/VwXEWOaqaqxxpe2rgMuiojaYgf8Nvcw47eLqsHMOpxWWiRyBTCsYHsXYFWjMmOAqfmE1h84XlJNRPxhW5X6EXlmlohotWcUzAb2lDQCWAmcDJxaWCAiRtSfV7oV+FNzCQ2c1MwsKbXO3M/8jKVJ5O5qVgK3RMQCSefmj0/Znnqd1Mwssda6ixgR04HpjfY1mcwi4svF1OmkZmaJZGE5bzOzBtKb0pzUzCwxUZHitYec1MwskVa8+1kSTmpmlli7XfnWzKwp6U1pTmpmllQrjVMrFSc1M0tEQKWTmpllSXpTmpOamW2HFDfUnNTMLJnckI70ZjUnNTNLzC01M8sQIbfUzCwrfPfTzLKlHT+h3cysSU5qZpYpvqZmZpmRWySy3FFsm5OamSXmlW/NLFPc/cyQR2Yt5Ls/nkZtXR2nf+4wvvGlYxscjwj+80d38/CshezUpTPX/9dp7D8y92jDDRvf45v/8zv+tuQNhPjJ907l4H1HcPZ3f8Xi19cA8PeNm+nZoxuP//qiNv9uHcExh+3NlRecRGVFBb++9ymuu+2hBsd79ejG5P86nRG79Of9LR9y3uV38NKrbwDw/L3f5933PqC2ro6amjqOPvPqcnyFsuuw3U9JtwCfBdZExD6lOk9bqq2t4+If3sVd13+dIQN68+mzfsj4I/dhrxGD68s8PGshS5av5Zm7/ou5C17jwqvvZMYtFwDwnz+extGH7s2vrvwqWz6sYfP7WwC4+Yqz6j9/yU/uoWf3rm37xTqIigpxzYVfYMKkyax6cz2P3vYdHpj5AouWrq4vc8FZn+GFl1dwxoW/YM9dB3LNRV/gxP/4af3xfz33J7y9YVM5wk+RdA++LeWqvLcC40tYf5ubt3AZu+1SzW5D+9O5UxUnHjuaB2a+0KDMgzNf4IvHj0USY/YZwYZ3N7N63QY2btrM088u5vTPHQZA505V9OqxU4PPRgT3PvIsE449qM2+U0dy0Md2Y8nydSxb+RYf1tQy7aF5HP+J/RqU2WvEIGbOXgTAK8veZPjgvlT37VGOcNMrP06tmFc5lCypRcRM4O1S1V8Ob6xdz9ABveu3hwzozRtrNzQqs4EhjcqsXruB11a+Rb8+3Tnv8jv45Jd+wDev+C2bNn/Q4LOznnuV6r492GP4gFJ+jQ5rcHUvVr75Tv32qjffYXB1rwZlXnxlJZ/95AEAjB61K8MG9a3/PSOCaZMn8djtF3LmhCPaKuxUUpGvcij78xMknSNpjqQ569auLXc4zYr4532Nm+HRRCEp13Wdv2gFZ038OI/dfhE7devC9bc/3KDcPX+ey0S30kqmqdVaG/9c1932EL177sTMOy7mnC9+gvkvr6C2tg6A8Wf/mHFn/IB/+8aNnH3SkRx+4B5tEXbqbJ0mVcyrHMqe1CLipogYExFj+ldXlzucZg0Z0JuVa9bXb69as55B1T3/qcyqRmUG9u/F4AG9GVLdm4P22Q2Afz36AOYvWl5frqamlvsfn8+Jxx5Yyq/Qoa1as56hA/vUbw8Z2IfV6xq2tDduep9Jl/2Go067inMvvZ3+vbuzbNVbAPVl173zLn96fD6jP7Zbm8WeOiluqpU9qbUnB+49nKXL17Js1Vts+bCGPzw0j/FH7tugzGeO3JffT3+GiGDOi0vp2b0rg/r3YmC/ngwZ2JvFy94E4InZi9hrxKD6z/1l9iI+stsAhgzog5XGvIXL2GN4NcOH9KNTVSUTjx3NAzPnNyjTs3s3OlVVAvClEw/nqWcXs3HT++zUtTPdd+oCwE5dO3P0oSN56dVVbf4d0kJF/lcOHtKRQFVVJVd++yS+8I0bqaur45TPHsrI3Qdz67QnAfjyxI9z7OGjePipBYw96TK6de3M9d87rf7zV15wEudeejsffljLrkP7NTh2z0Pz3PUssdraOi68+k7uvv7rVFaKO+57mr8tWc1ZEz8OwK+mPcleIwbxs/8+g9q6OhYtXc15l98BQHW/Hvzm6n8HoLKqkrsfnMMjs14q23cptxSPvUVNXQNqlYql3wHjgP7Am8ClEfHL5j4z+qAxMfOpZ0oSj5VG9aHnlzsES+CDRXdS996aHUpJe+97YNx+7+NFlR27R++5ETFmR86XVMlaahFxSqnqNrMyS3FLzd1PM0tE8txPM8uY9KY0JzUz2x4pzmpOamaWULrnfjqpmVliKb6k5qRmZskIJzUzyxh3P80sU9xSM7NMSXFO84R2M0uo2BU6ish8ksZLWiRpsaSLmzh+mqT5+ddTkvZvqU631Mwssda4piapErgBOBZYAcyWdF9ELCwothT4RES8I+k44CbgkObqdVIzs0Ra8cErY4HFEbEEQNJU4ASgPqlFxFMF5Z8GdmmpUnc/zSy51ul+DgWWF2yvyO/blq8CD7RUqVtqZpZYgu5nf0lzCrZvioib6qv5Z02uhSbpk+SS2sdbOqGTmpkllmBIx7pm1lNbAQwr2N4F+KflhCXtB9wMHBcRb7V0Qnc/zSyxVrr5ORvYU9IISZ2Bk4H7GpxHGg5MA86IiJeLic0tNTNLrhVuFEREjaRJwAygErglIhZIOjd/fApwCdAPuDH/NLCallbSdVIzs0Rac5HIiJgOTG+0b0rB+7OBs5PU6aRmZomleUaBk5qZJZfirOakZmYJeZFIM8sYr9JhZpnhRSLNLHPc/TSzTHFLzcwyJcU5zUnNzBKSW2pmljnpzWpOamaWSCsuElkSTmpmlpi7n2aWKR7SYWbZkt6c5qRmZsmlOKc5qZlZMvKQDjPLGqU4qzmpmVli6U1pTmpmth1S3FBzUjOzpLxIpJlliNdTM7PMcVIzs0xx99PMssPj1MwsS4SHdJhZ1qQ4qzmpmVlivqZmZpniRSLNLFuc1MwsS9z9NLPMSPuMAkVEuWOoJ2ktsKzccZRAf2BduYOwRLL6m+0aEdU7UoGkB8n9+RRjXUSM35HzJZWqpJZVkuZExJhyx2HF82/WflWUOwAzs9bkpGZmmeKk1jZuKncAlph/s3bK19TMLFPcUjOzTHFSM7NMcVIrIUnjJS2StFjSxeWOx1om6RZJayS9WO5YbPs4qZWIpErgBuA4YBRwiqRR5Y3KinAr0KaDRa11OamVzlhgcUQsiYgtwFTghDLHZC2IiJnA2+WOw7afk1rpDAWWF2yvyO8zsxJyUiudpqb8evyMWYk5qZXOCmBYwfYuwKoyxWLWYTiplc5sYE9JIyR1Bk4G7itzTGaZ56RWIhFRA0wCZgAvAXdGxILyRmUtkfQ7YBawl6QVkr5a7pgsGU+TMrNMcUvNzDLFSc3MMsVJzcwyxUnNzDLFSc3MMsVJrR2RVCvpOUkvSrpL0k47UNetkk7Kv7+5ucn2ksZJOnw7zvGapH966tC29jcq827Cc/23pG8njdGyx0mtfdkcEQdExD7AFuDcwoP5lUESi4izI2JhM0XGAYmTmlk5OKm1X08AH8m3oh6T9FvgBUmVkq6RNFvSfElfA1DOZEkLJd0PDNhakaTHJY3Jvx8vaZ6k5yU9Imk3csnzW/lW4pGSqiXdnT/HbElH5D/bT9KfJT0r6ec0Pf+1AUl/kDRX0gJJ5zQ6dm0+lkckVef37SHpwfxnnpA0slX+NC0z/IT2dkhSFbl12h7M7xoL7BMRS/OJYUNEHCypC/BXSX8GDgT2AvYFBgILgVsa1VsN/AI4Kl9X34h4W9IU4N2I+GG+3G+BH0fEk5KGk5s1sTdwKfBkRFwm6V+ABklqG76SP0c3YLakuyPiLWBnYF5EXCDpknzdk8g9EOXciHhF0iHAjcDR2/HHaBnlpNa+dJP0XP79E8AvyXULn4mIpfn9nwb223q9DOgF7AkcBfwuImqBVZIebaL+Q4GZW+uKiG2tK/YpYJRU3xDrKalH/hwT85+9X9I7RXyn8yVNyL8flo/1LaAO+H1+/2+AaZK657/vXQXn7lLEOawDcVJrXzZHxAGFO/J/uTcV7gLOi4gZjcodT8tLH6mIMpC7bHFYRGxuIpai591JGkcuQR4WEe9Jehzouo3ikT/v+sZ/BmaFfE0te2YA/0dSJwBJH5W0MzATODl/zW0w8MkmPjsL+ISkEfnP9s3v3wj0KCj3Z3JdQfLlDsi/nQmclt93HNCnhVh7Ae/kE9pIci3FrSqAra3NU8l1a/8OLJX0b/lzSNL+LZzDOhgntey5mdz1snn5h4f8nFyL/B7gFeAF4GfAXxp/MCLWkrsONk3S8/yj+/dHYMLWGwXA+cCY/I2IhfzjLuz3gaMkzSPXDX69hVgfBKokzQcuB54uOLYJ+JikueSumV2W338a8NV8fAvwEunWiFfpMLNMcUvNzDLFSc3MMsVJzcwyxUnNzDLFSc3MMsVJzcwyxUnNzDLl/wNSXumz+tTm5QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recover our model\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df = 0.8, max_features = 4000, stop_words = 'english')\n",
    "X_vec_train = tfidf.fit_transform(X_train)\n",
    "X_vec_test = tfidf.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(C=1, l1_ratio=0.1, max_iter=10000, penalty='elasticnet',\n",
    "                   solver='saga')\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "l1_estimators = [('logreg', logreg), ('mnb', mnb)]\n",
    "\n",
    "stack = StackingClassifier(l1_estimators)\n",
    "evaluation(stack, 'TfidfVectorizer + Logreg/MNB stack', X_vec_train, y_train, X_vec_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks good!\n",
    "- We're ready to merge our predictions back into the test data set, check out a few misclassifications, and find our top terms.\n",
    "- Reminder that subreddit code 0 = classicalmusic, subreddit code 1 = jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>Pieces for saxophone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>Remote Music Lessons Survey I know this is tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>What are some of the greatest duos in Jazz his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>Two great of the classical Hungarian music and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>Does anybody know the name to this song I've b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                all_text\n",
       "18024                               Pieces for saxophone\n",
       "4302   Remote Music Lessons Survey I know this is tan...\n",
       "2187   What are some of the greatest duos in Jazz his...\n",
       "16216  Two great of the classical Hungarian music and...\n",
       "11327  Does anybody know the name to this song I've b..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.DataFrame(X_test, columns = ['all_text'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge our predicted values\n",
    "\n",
    "X_test['pred_subreddit_code'] = stack.predict(X_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>pred_subreddit_code</th>\n",
       "      <th>pred_subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>Pieces for saxophone</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>Remote Music Lessons Survey I know this is tan...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>What are some of the greatest duos in Jazz his...</td>\n",
       "      <td>1</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>Two great of the classical Hungarian music and...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>Does anybody know the name to this song I've b...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                all_text  pred_subreddit_code  \\\n",
       "18024                               Pieces for saxophone                    0   \n",
       "4302   Remote Music Lessons Survey I know this is tan...                    0   \n",
       "2187   What are some of the greatest duos in Jazz his...                    1   \n",
       "16216  Two great of the classical Hungarian music and...                    0   \n",
       "11327  Does anybody know the name to this song I've b...                    0   \n",
       "\n",
       "       pred_subreddit  \n",
       "18024  classicalmusic  \n",
       "4302   classicalmusic  \n",
       "2187             jazz  \n",
       "16216  classicalmusic  \n",
       "11327  classicalmusic  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate our codes back into subreddits\n",
    "\n",
    "X_test['pred_subreddit'] = X_test['pred_subreddit_code'].map({0: 'classicalmusic', 1: 'jazz'})\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>pred_subreddit_code</th>\n",
       "      <th>pred_subreddit</th>\n",
       "      <th>true_subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>Pieces for saxophone</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>Remote Music Lessons Survey I know this is tan...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>What are some of the greatest duos in Jazz his...</td>\n",
       "      <td>1</td>\n",
       "      <td>jazz</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>Two great of the classical Hungarian music and...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>Does anybody know the name to this song I've b...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                all_text  pred_subreddit_code  \\\n",
       "18024                               Pieces for saxophone                    0   \n",
       "4302   Remote Music Lessons Survey I know this is tan...                    0   \n",
       "2187   What are some of the greatest duos in Jazz his...                    1   \n",
       "16216  Two great of the classical Hungarian music and...                    0   \n",
       "11327  Does anybody know the name to this song I've b...                    0   \n",
       "\n",
       "       pred_subreddit  true_subreddit  \n",
       "18024  classicalmusic  classicalmusic  \n",
       "4302   classicalmusic            jazz  \n",
       "2187             jazz            jazz  \n",
       "16216  classicalmusic  classicalmusic  \n",
       "11327  classicalmusic  classicalmusic  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge our true values back in so we can look at a few misclassified items\n",
    "\n",
    "X_test['true_subreddit'] = df['subreddit']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>pred_subreddit_code</th>\n",
       "      <th>pred_subreddit</th>\n",
       "      <th>true_subreddit</th>\n",
       "      <th>pred_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>Pieces for saxophone</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>Remote Music Lessons Survey I know this is tan...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>jazz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>What are some of the greatest duos in Jazz his...</td>\n",
       "      <td>1</td>\n",
       "      <td>jazz</td>\n",
       "      <td>jazz</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>Two great of the classical Hungarian music and...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>Does anybody know the name to this song I've b...</td>\n",
       "      <td>0</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>classicalmusic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                all_text  pred_subreddit_code  \\\n",
       "18024                               Pieces for saxophone                    0   \n",
       "4302   Remote Music Lessons Survey I know this is tan...                    0   \n",
       "2187   What are some of the greatest duos in Jazz his...                    1   \n",
       "16216  Two great of the classical Hungarian music and...                    0   \n",
       "11327  Does anybody know the name to this song I've b...                    0   \n",
       "\n",
       "       pred_subreddit  true_subreddit  pred_correct  \n",
       "18024  classicalmusic  classicalmusic          True  \n",
       "4302   classicalmusic            jazz         False  \n",
       "2187             jazz            jazz          True  \n",
       "16216  classicalmusic  classicalmusic          True  \n",
       "11327  classicalmusic  classicalmusic          True  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column to indicate correct/incorrect predictions\n",
    "\n",
    "X_test['pred_correct'] = X_test['pred_subreddit'] == X_test['true_subreddit']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_text               170\n",
       "pred_subreddit_code    170\n",
       "pred_subreddit         170\n",
       "true_subreddit         170\n",
       "pred_correct           170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How did our misclassifications compare between genres?\n",
    "\n",
    "# jazz misclassified as classical\n",
    "X_test[(X_test['pred_correct'] == False) & (X_test['true_subreddit'] == 'jazz')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_text               122\n",
       "pred_subreddit_code    122\n",
       "pred_subreddit         122\n",
       "true_subreddit         122\n",
       "pred_correct           122\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classical misclassified as jazz\n",
    "X_test[(X_test['pred_correct'] == False) & (X_test['true_subreddit'] == 'classicalmusic')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's export some of these so we can take a look later\n",
    "X_test[(X_test['pred_correct'] == False) & (X_test['true_subreddit'] == 'jazz')].head(10).to_csv('../data/jazz_misclass.csv')\n",
    "X_test[(X_test['pred_correct'] == False) & (X_test['true_subreddit'] == 'classicalmusic')].head(10).to_csv('../data/classical_misclass.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- The model was almost 1.5x as likely to misclassify jazz as it was to misclassify classical.\n",
    "- We have a sample of 10 of each kind of misclassification for inclusion in our presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's get our top terms from each genre.\n",
    "- We won't distinguish between correct and incorrect predictions here - we want a realistic view of things, so we'll include both.\n",
    "- Let's try for the top 10 words and top 10 bigrams from each.\n",
    "- Probably easiest to use CountVectorizer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish our genre datasets\n",
    "jazz = X_test[X_test['pred_subreddit'] == 'jazz']\n",
    "classical = X_test[X_test['pred_subreddit'] == 'classicalmusic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our CountVectorizer\n",
    "cv = CountVectorizer(stop_words = 'english', ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out jazz\n",
    "jazz_vec = cv.fit_transform(jazz['all_text'])\n",
    "jazz_df = pd.DataFrame(jazz_vec.todense(), columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jazz       2753\n",
       "like        936\n",
       "music       719\n",
       "know        637\n",
       "just        620\n",
       "ve          515\n",
       "really      437\n",
       "looking     420\n",
       "play        372\n",
       "new         363\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words\n",
    "\n",
    "jazz_df[[col for col in jazz_df if len(col.split()) == 1]].sum().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miles davis       136\n",
       "don know           92\n",
       "big band           90\n",
       "jazz music         75\n",
       "does know          69\n",
       "new jazz           67\n",
       "john coltrane      60\n",
       "jazz albums        58\n",
       "charlie parker     55\n",
       "feel like          54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 bigrams\n",
    "\n",
    "jazz_df[[col for col in jazz_df if len(col.split()) == 2]].sum().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out classical\n",
    "classical_vec = cv.fit_transform(classical['all_text'])\n",
    "classical_df = pd.DataFrame(classical_vec.todense(), columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "music        1824\n",
       "classical    1108\n",
       "like          803\n",
       "piece         676\n",
       "pieces        639\n",
       "piano         552\n",
       "know          545\n",
       "just          510\n",
       "ve            426\n",
       "help          355\n",
       "dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words\n",
    "\n",
    "classical_df[[col for col in classical_df if len(col.split()) == 1]].sum().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classical music     600\n",
       "don know             84\n",
       "piano concerto       81\n",
       "sheet music          55\n",
       "th century           51\n",
       "feel like            51\n",
       "does know            50\n",
       "ve heard             49\n",
       "thanks advance       46\n",
       "classical pieces     45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 bigrams\n",
    "\n",
    "classical_df[[col for col in classical_df if len(col.split()) == 2]].sum().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- We now have our final results, which are the top 10 words and bigrams used in each genre.\n",
    "- The words aren't as useful - they're mostly pretty generic musical terms, and the presence of \"classical\" and \"jazz\" doesn't seem very helpful either.\n",
    "- The bigrams are pretty telling, particularly in the jazz genre - we've got a few names of jazz musicians in there, along with \"big band.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL NOTES/NEXT STEPS**:\n",
    "- We've achieved our initial goal of predicting genres from posts and then extracting top terms from our predicted genres, using an optimized model that balances accuracy, overfitting, and speed pretty well.\n",
    "- Looking at our extracted top terms, there are definitely a few useful gems in there, but we probably need to spend more time building stop words lists to make these as useful as possible.\n",
    "- Other modeling options we could consider: unsupervised learning to cluster by genre, or principal component analysis (PCA) to reduce dimensionality.\n",
    "- Making the model more useful: it's probably time to try a multi-class problem, since that would be more representative of the final application.\n",
    "- See [Experiments](./04a_experiments.ipynb) for further work in these directions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c5d815f4904d5c36e1cb6a23cb867a61c9881b6acdb1b6d63422ceae43ed5d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
